---
layout: single
title: "[Pytorch] MNIST from scratch"
categories: Pytorch
tag: [python, pytorch]
author_profile: true
sidebar_main: true
---

## Intro

MNIST is one of the most popular datasets out there for deep learning. It consists of images of `10 digits`: from 0 to 9. Let's take a look.

![MNIST](../../assets/images/MNIST/mnist.png)

## Get Data

Instead of getting it from `torchvision.datasets.MNIST`, we'll download data through `Kaggle API`. You can find the detailed documentation [here](https://github.com/Kaggle/kaggle-api)

Once you set up your Kaggle API. Let's download the MNIST dataset!

1. Fire up your terminal and go to the project folder
2. Enter `kaggle competitions download -c digit-recognizer`
3. Now you'll see there's a `digit-recognizer.zip` file.
4. Let's extract it using `unzip digit-recognizer.zip`
5. Let's move all the extracted files to `data/` directory by `mkdir data`, `mv *.csv data`, `mv *.zip data`.

## Folder Structure

- `data/`
- `dataset.py`
- `model.py`
- `train.py`
- `utils.py`

---

Now, let's analyze codes!

## `dataset.py`

```python
import os
import torch
import pandas as pd
from torch.utils.data import Dataset

class MNISTDataset(Dataset):
    def __init__(self, image_dir, file_name, transform=None, train=True):
        self.image_dir = image_dir
        self.transform = transform
        self.train = train
        self.df = pd.read_csv(os.path.join(image_dir, file_name))

        if self.train:
            self.X = self.df.iloc[:, 1:].to_numpy().astype('uint8') # Need to convert to uint8 dtype for transforms.ToTensor()
            self.y = torch.tensor(self.df.iloc[:, 0].to_numpy(), dtype=torch.long)
        else:
            self.X = self.df.iloc[:,:].to_numpy().astype('uint8')
            self.y = None

        # 784 = 1 x 28 x 28
        self.X = self.X.reshape(len(self.X),28,28)

        # transform
        if transform:
            self.X = transform(self.X.transpose(1, 2, 0)) # convert to (H,W,C)

        self.classes = 10
        self.data = self.X

    def __getitem__(self, idx):
        X = self.X[idx].unsqueeze(0) # (1,28,28) fit dimension
        y = self.y[idx] if self.train else None

        return X, y

    def __len__(self):
        return len(self.X)

```

---

## `model.py`

```python
import torch
import torch.nn as nn

class MNIST_CNN(nn.Module):
    def __init__(self, in_channels=1, num_classes=10):
        super(MNIST_CNN, self).__init__()

        # Conv Layers
        self.layer1 = nn.Sequential(
            nn.Conv2d(in_channels,8,3,1,1, bias = True),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2,2)
        )

        self.layer2 = nn.Sequential(
            nn.Conv2d(8,16,3,1,1, bias = True),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2,2)
        )

        # FC layer
        self.layer3 = nn.Sequential(
            # nn.Flatten(),
            nn.Linear(16 * 7 * 7, num_classes, bias = True)
        )

    def forward(self, x):
        # Conv Layers
        out = self.layer1(x)
        out = self.layer2(out)

        # FC layer
        out = out.reshape(out.shape[0], -1)
        out = self.layer3(out)

        return out


```

---

## `utils.py`

```python
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# Check Accuracy
def check_accuracy(loader, model):
    num_correct = 0
    num_samples = 0
    model.eval() # Set to evaluation mode

    with torch.no_grad():
        for x, y in loader:
            x = x.to(device=device)
            y = y.to(device=device)

            scores = model(x)
            _, preds = scores.max(dim = 1)

            num_correct += (preds == y).sum()
            num_samples += scores.size(0) # batch size

        accuracy = float(num_correct) / float(num_samples) * 100

    return accuracy
```

---

## `train.py`

```python
import torch
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms

from dataset import MNISTDataset
from model import MNIST_CNN

from utils import check_accuracy
import numpy as np

# Set Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Model Architecture Settings
in_channels = 1
num_classes = 10

# Hyperparameters
BATCH_SIZE = 64
NUM_EPOCHS = 10
LEARNING_RATE = 1e-3

# Load Datasets
MNIST_dataset_train = MNISTDataset('./data',
                                   'train.csv',
                                   transform = transforms.ToTensor())

MNIST_dataset_test = datasets.MNIST(root='data/',
                                    train=False,
                                    transform = transforms.ToTensor(),
                                    download=True)

MNIST_dataloader_train = DataLoader(dataset = MNIST_dataset_train,
                                    batch_size = BATCH_SIZE,
                                    shuffle = True,
                                    drop_last = True
                                    )
MNIST_dataloader_test = DataLoader(dataset = MNIST_dataset_test,
                                   batch_size = BATCH_SIZE,
                                   shuffle = True,
                                   drop_last = False # test set -> don't need to
                                   )

# Import model
model = MNIST_CNN(in_channels=1, num_classes=num_classes).to(device)

# Loss / Optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# Train Neural Network
def train_model(model, loss_fn, optimizer, train_loader, test_loader, num_epochs=NUM_EPOCHS, print_every_epoch = 1):
    model.train() # Set to training mode

    for epoch in range(num_epochs):
        for batch_idx, (images, labels) in enumerate(train_loader):
            # [1] Zero out gradients
            optimizer.zero_grad()

            # [2] Forwardprop
            scores = model(images)
            loss = loss_fn(scores, labels)

            # [3] Backprop
            loss.backward()

            # [4] Gradient Descent
            optimizer.step()

        # Print Accuracy
        print(f"Epoch {epoch+1}| Test Accuracy: {check_accuracy(test_loader,model):.3f}% | Train Accuracy: {check_accuracy(train_loader, model):.3f}%")


# Start Training
if __name__ == "__main__":
    train_model(model, loss_fn, optimizer, MNIST_dataloader_train, MNIST_dataloader_test)
```
